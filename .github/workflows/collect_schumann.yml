name: Collect Schumann Data

on:
  schedule:
    - cron: '0 * * * *'  # –ö–∞–∂–¥—ã–π —á–∞—Å
  workflow_dispatch:

jobs:
  schumann:
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout repo
        uses: actions/checkout@v3

      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt
          pip install beautifulsoup4 requests

      - name: üöÄ Fetch Schumann point
        env:
          TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.CHANNEL_ID }}
        run: |
          python - <<EOF
          import json
          import time
          import requests
          import re
          from pathlib import Path
          from bs4 import BeautifulSoup

          def send_telegram_message(token, chat_id, message):
              if not token or not chat_id:
                  print("Telegram token or chat_id missing")
                  return
              try:
                  url = f"https://api.telegram.org/bot{token}/sendMessage"
                  data = {"chat_id": chat_id, "text": message}
                  response = requests.post(url, data=data)
                  response.raise_for_status()
                  print("Telegram notification sent")
              except Exception as e:
                  print(f"Failed to send Telegram notification: {e}")

          def _fetch_schumann_data(url, attempts=5, backoff=2.0, timeout=45):
              for i in range(attempts):
                  try:
                      response = requests.get(url, timeout=timeout, headers={"User-Agent": "VayboMeter/1.0"})
                      response.raise_for_status()
                      data = response.text
                      print(f"Raw response from {url} (first 200 chars): {data[:200]}...")
                      if data:
                          print(f"Success: Got data from {url}")
                          return data
                      print(f"Retry {i+1}/{attempts} for {url} after {backoff**i}s")
                      time.sleep(backoff**i)
                  except Exception as e:
                      print(f"Retry {i+1}/{attempts} failed for {url}: {e}")
                      time.sleep(backoff**i)
              print(f"All attempts failed for {url}")
              return ""

          try:
              # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
              freq = None
              amp = None

              # –ü–∞—Ä—Å–∏–Ω–≥ sosrff.tsu.ru –∏ –Ω–æ–≤—ã–π –∞–¥—Ä–µ—Å
              if not (freq and amp):
                  for page in ["https://sosrff.tsu.ru", "http://sos70.ru"]:
                      raw_html = _fetch_schumann_data(page)
                      if raw_html:
                          soup = BeautifulSoup(raw_html, "html.parser")
                          # –ü–æ–∏—Å–∫ JSON –≤ <script>
                          scripts = soup.find_all("script", type="text/javascript")
                          for script in scripts:
                              script_text = script.text.strip()
                              print(f"Analyzing script on {page}: {script_text[:200]}...")
                              json_match = re.search(r"\{.*?\bfrequency\s*:\s*(\d+\.?\d*)\s*,\s*amplitude\s*:\s*(\d+\.?\d*)\s*}", script_text, re.DOTALL)
                              if json_match:
                                  freq = json_match.group(1)
                                  amp = json_match.group(2)
                                  print(f"Found in JSON on {page}: freq={freq}, amp={amp}")
                                  break
                          if freq and amp:
                              break
                          # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
                          all_text = " ".join(soup.stripped_strings)
                          print(f"Searching text on {page}: {all_text[:200]}...")
                          freq_match = re.search(r"(\d+\.?\d*)\s*Hz", all_text)
                          amp_match = re.search(r"(\d+\.?\d*)\s*nT", all_text)
                          freq = freq_match.group(1) if freq_match else None
                          amp = amp_match.group(1) if amp_match else None
                          print(f"Parsed from {page}: freq={freq}, amp={amp}")
                          # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                          if not (freq and amp):
                              img_tags = soup.find_all("img")
                              for img in img_tags:
                                  img_src = img.get("src", "")
                                  if "schumann" in img_src.lower() or "resonance" in img_src.lower():
                                      print(f"Found potential Schumann image: {img_src}")

              # –ü–∞—Ä—Å–∏–Ω–≥ HeartMath
              if not (freq and amp):
                  for url in ["https://www.heartmath.org/gci/gci-data/", "https://nocc.heartmath.org/power_levels/public/charts/power_levels.html"]:
                      raw_html = _fetch_schumann_data(url)
                      if raw_html:
                          soup = BeautifulSoup(raw_html, "html.parser")
                          all_text = " ".join(soup.stripped_strings)
                          print(f"Searching text on HeartMath: {all_text[:200]}...")
                          freq_match = re.search(r"(\d+\.?\d*)\s*Hz", all_text)
                          amp_match = re.search(r"(\d+\.?\d*)\s*nT", all_text)
                          freq = freq_match.group(1) if freq_match else None
                          amp = amp_match.group(1) if amp_match else None
                          print(f"Parsed from HeartMath: freq={freq}, amp={amp}")
                          if freq and amp:
                              break

              # –ü–∞—Ä—Å–∏–Ω–≥ BGS Geomagnetism
              if not (freq and amp):
                  raw_html = _fetch_schumann_data("https://geomag.bgs.ac.uk/")
                  if raw_html:
                      soup = BeautifulSoup(raw_html, "html.parser")
                      all_text = " ".join(soup.stripped_strings)
                      print(f"Searching text on BGS: {all_text[:200]}...")
                      freq_match = re.search(r"(\d+\.?\d*)\s*Hz", all_text)
                      amp_match = re.search(r"(\d+\.?\d*)\s*pT", all_text)
                      freq = freq_match.group(1) if freq_match else None
                      amp = amp_match.group(1) if amp_match else None
                      print(f"Parsed from BGS: freq={freq}, amp={amp}")

              # –ü–∞—Ä—Å–∏–Ω–≥ Sierra Nevada
              if not (freq and amp):
                  raw_html = _fetch_schumann_data("https://www.sciencedirect.com/science/article/pii/S1364682618301200")
                  if raw_html:
                      soup = BeautifulSoup(raw_html, "html.parser")
                      all_text = " ".join(soup.stripped_strings)
                      print(f"Searching text on Sierra Nevada: {all_text[:200]}...")
                      freq_match = re.search(r"(\d+\.?\d*)\s*Hz", all_text)
                      amp_match = re.search(r"(\d+\.?\d*)\s*nT", all_text)
                      freq = freq_match.group(1) if freq_match else None
                      amp = amp_match.group(1) if amp_match else None
                      print(f"Parsed from Sierra Nevada: freq={freq}, amp={amp}")

              # –ü–∞—Ä—Å–∏–Ω–≥ Chinese ELF
              if not (freq and amp):
                  raw_html = _fetch_schumann_data("http://www.geophys.ac.cn/")
                  if raw_html:
                      soup = BeautifulSoup(raw_html, "html.parser")
                      all_text = " ".join(soup.stripped_strings)
                      print(f"Searching text on Chinese ELF: {all_text[:200]}...")
                      freq_match = re.search(r"(\d+\.?\d*)\s*Hz", all_text)
                      amp_match = re.search(r"(\d+\.?\d*)\s*nT", all_text)
                      freq = freq_match.group(1) if freq_match else None
                      amp = amp_match.group(1) if amp_match else None
                      print(f"Parsed from Chinese ELF: freq={freq}, amp={amp}")

              if not (freq and amp):
                  # Fallback –Ω–∞ –∫—ç—à
                  cache = Path("schumann_hourly.json")
                  if cache.exists():
                      arr = json.loads(cache.read_text())
                      if arr:
                          last = arr[-1]
                          freq, amp = last["freq"], last["amp"]
                          print("Using cached data:", last)
                      else:
                          print("Cache exists but is empty")
                  else:
                      print("No cache file found")

              if freq and amp:
                  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ—á–∫—É
                  rec = {"ts": int(time.time()), "freq": float(freq), "amp": float(amp)}
                  cache = Path("schumann_hourly.json")
                  arr = json.loads(cache.read_text()) if cache.exists() else []
                  arr.append(rec)
                  cutoff = int(time.time()) - 7*24*3600
                  arr = [r for r in arr if r["ts"] >= cutoff]
                  cache.write_text(json.dumps(arr, ensure_ascii=False))
                  print("Saved Schumann point:", rec)
                  # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∞–Ω–æ–º–∞–ª–∏—è—Ö
                  if float(freq) > 8.0 or float(amp) > 100.0:
                      send_telegram_message(
                          "${{ env.TELEGRAM_TOKEN }}",
                          "${{ env.TELEGRAM_CHAT_ID }}",
                          f"üö® High Schumann! Freq: {freq} Hz, Amp: {amp}"
                      )
              else:
                  print("No valid freq/amp data received, even from cache")
          except Exception as e:
              print(f"Error fetching Schumann data: {e}")
              send_telegram_message(
                  "${{ env.TELEGRAM_TOKEN }}",
                  "${{ env.TELEGRAM_CHAT_ID }}",
                  f"‚ö†Ô∏è Schumann script error: {e}"
              )
          EOF

      - name: üóÇÔ∏è Commit cache
        uses: stefanzweifel/git-auto-commit-action@v4
        with:
          commit_message: "schumann: add hourly point"
          file_pattern: "schumann_hourly.json"