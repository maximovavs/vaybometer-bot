#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
gpt.py (Cyprus)

–û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –º–æ–¥—É–ª—å –∏–∑ KLD:
‚Ä¢ gpt_complete(): –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –ø–æ –æ—á–µ—Ä–µ–¥–∏ ‚Äî OpenAI ‚Üí Gemini(HTTP) ‚Üí Groq.
‚Ä¢ –ë—ã—Å—Ç—Ä–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ 429/–∫–≤–æ—Ç–µ.
‚Ä¢ gpt_blurb(culprit) ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (summary, tips[3]) —Å —Ñ–æ–ª–ª–±—ç–∫–∞–º–∏ –±–µ–∑ –∫–ª—é—á–µ–π.

–°–µ–∫—Ä–µ—Ç—ã: OPENAI_API_KEY, GEMINI_API_KEY, GROQ_API_KEY.
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: openai (–æ–ø—Ü.), requests.
"""

from __future__ import annotations
import os
import random
import logging
from typing import Tuple, List, Optional

log = logging.getLogger(__name__)

# ‚îÄ‚îÄ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from openai import OpenAI  # –µ–¥–∏–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è OpenAI –∏ Groq (—á–µ—Ä–µ–∑ base_url)
except Exception:
    OpenAI = None  # type: ignore

try:
    import requests  # –¥–ª—è Gemini HTTP API
except Exception:
    requests = None  # type: ignore

OPENAI_KEY = os.getenv("OPENAI_API_KEY") or ""
GEMINI_KEY = os.getenv("GEMINI_API_KEY") or ""
GROQ_KEY   = os.getenv("GROQ_API_KEY") or ""

PROVIDER_ORDER = [p for p in ("openai", "gemini", "groq")]

# —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Groq ‚Äî –ø—Ä–æ–±—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫—É
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "gemma2-9b-it",
    "deepseek-r1-distill-llama-70b",
]

# ‚îÄ‚îÄ –∫–ª–∏–µ–Ω—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _openai_client() -> Optional["OpenAI"]:
    if not OPENAI_KEY or not OpenAI:
        return None
    try:
        # –±–µ–∑ —Ä–µ—Ç—Ä–∞–µ–≤, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ
        return OpenAI(api_key=OPENAI_KEY, timeout=20.0, max_retries=0)
    except Exception as e:
        log.warning("OpenAI client init error: %s", e)
        return None

def _groq_client() -> Optional["OpenAI"]:
    if not GROQ_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=GROQ_KEY, base_url="https://api.groq.com/openai/v1", timeout=25.0, max_retries=0)
    except Exception as e:
        log.warning("Groq client init error: %s", e)
        return None

# ‚îÄ‚îÄ –æ–±—â–∏–π –≤—ã–∑–æ–≤ LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def gpt_complete(
    prompt: str,
    system: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 600,
) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–ª–∏ "".
    –ü–æ—Ä—è–¥–æ–∫: OpenAI ‚Üí Gemini ‚Üí Groq.
    """
    text = ""

    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    # 1) OpenAI
    if "openai" in PROVIDER_ORDER and not text:
        cli = _openai_client()
        if cli:
            try:
                r = cli.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                text = (r.choices[0].message.content or "").strip()
            except Exception as e:
                msg = str(e).lower()
                if any(k in msg for k in ("rate limit", "insufficient_quota", "429")):
                    log.warning("OpenAI rate/quota issue ‚Üí fallback: %s", e)
                else:
                    log.warning("OpenAI error: %s", e)
                text = ""

    # 2) Gemini (HTTP)
    if "gemini" in PROVIDER_ORDER and not text and GEMINI_KEY and requests:
        try:
            full_prompt = f"{system.strip()}\n\n{prompt}" if system else prompt
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
            resp = requests.post(
                url,
                params={"key": GEMINI_KEY},
                json={
                    "contents": [{"parts": [{"text": full_prompt}]}],
                    "generationConfig": {"temperature": temperature, "maxOutputTokens": max_tokens},
                },
                timeout=25,
            )
            if resp.status_code == 200:
                data = resp.json()
                cand = (data.get("candidates") or [{}])[0]
                parts = ((cand.get("content") or {}).get("parts") or [])
                text = "".join(p.get("text", "") for p in parts).strip()
            else:
                log.warning("Gemini HTTP %s: %s", resp.status_code, resp.text[:200])
        except Exception as e:
            log.warning("Gemini exception: %s", e)

    # 3) Groq (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API)
    if "groq" in PROVIDER_ORDER and not text:
        cli = _groq_client()
        if cli:
            for mdl in GROQ_MODELS:
                try:
                    r = cli.chat.completions.create(
                        model=mdl,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                    text = (r.choices[0].message.content or "").strip()
                    if text:
                        break
                except Exception as e:
                    msg = str(e).lower()
                    if "not found" in msg or "decommissioned" in msg:
                        log.warning("Groq model %s unavailable, try next.", mdl)
                        continue
                    if "rate limit" in msg or "429" in msg:
                        log.warning("Groq rate limit on %s, try next.", mdl)
                        continue
                    log.warning("Groq error on %s: %s", mdl, e)
                    continue

    return text or ""

# ‚îÄ‚îÄ —Ñ–æ–ª–ª–±—ç–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CULPRITS = {
    "—Ç—É–º–∞–Ω": {
        "emoji": "üåÅ",
        "tips": ["üî¶ –°–≤–µ—Ç–ª–∞—è –æ–¥–µ–∂–¥–∞ –∏ —Ñ–æ–Ω–∞—Ä—å", "üöó –í–æ–¥–∏—Ç–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ", "‚è∞ –ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –ø–æ–µ–∑–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ", "üï∂Ô∏è –û—á–∫–∏ –ø—Ä–æ—Ç–∏–≤ –±–ª–∏–∫–æ–≤"],
    },
    "–º–∞–≥–Ω–∏—Ç–Ω—ã–µ –±—É—Ä–∏": {
        "emoji": "üß≤",
        "tips": ["üßò 5-–º–∏–Ω—É—Ç–Ω–∞—è –¥—ã—Ö–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞", "üåø –¢—ë–ø–ª—ã–π —Ç—Ä–∞–≤—è–Ω–æ–π —á–∞–π", "üôÖ –ú–µ–Ω—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π", "üòå –†–∞—Å—Ç—è–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–Ω–æ–º"],
    },
    "–Ω–∏–∑–∫–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ": {
        "emoji": "üå°Ô∏è",
        "tips": ["üíß –ü–µ–π—Ç–µ –≤–æ–¥—É", "üò¥ 20 –º–∏–Ω—É—Ç –æ—Ç–¥—ã—Ö–∞", "ü§∏ –õ—ë–≥–∫–∞—è –∑–∞—Ä—è–¥–∫–∞", "ü•ó –ú–µ–Ω—å—à–µ —Å–æ–ª–∏ –≤–µ—á–µ—Ä–æ–º"],
    },
    "—à–∞–ª—å–Ω–æ–π –≤–µ—Ç–µ—Ä": {
        "emoji": "üí®",
        "tips": ["üß£ –®–∞—Ä—Ñ —Å —Å–æ–±–æ–π", "üö∂ –ù–µ–±–æ–ª—å—à–∞—è –ø—Ä–æ–≥—É–ª–∫–∞", "üï∂Ô∏è –ó–∞—â–∏—Ç–∞ –≥–ª–∞–∑", "üå≥ –ò–∑–±–µ–≥–∞–π—Ç–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–µ—Å—Ç"],
    },
    "–∂–∞—Ä–∞": {
        "emoji": "üî•",
        "tips": ["üí¶ –ë—É—Ç—ã–ª–∫–∞ –≤–æ–¥—ã –ø–æ–¥ —Ä—É–∫–æ–π", "üß¢ –ì–æ–ª–æ–≤–Ω–æ–π —É–±–æ—Ä", "üå≥ –¢–µ–Ω—å –≤ –ø–æ–ª–¥–µ–Ω—å", "‚ùÑÔ∏è –ü—Ä–æ—Ö–ª–∞–¥–Ω—ã–π –∫–æ–º–ø—Ä–µ—Å—Å"],
    },
    "—Å—ã—Ä–æ—Å—Ç—å": {
        "emoji": "üíß",
        "tips": ["üëü –°–º–µ–Ω–Ω–∞—è –æ–±—É–≤—å", "üåÇ –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –∑–æ–Ω—Ç", "üå¨Ô∏è –ü—Ä–æ–≤–µ—Ç—Ä–∏–≤–∞–π—Ç–µ –¥–æ–º", "üß• –ù–µ–ø—Ä–æ–º–æ–∫–∞–µ–º–∞—è –∫—É—Ä—Ç–∫–∞"],
    },
    "–ø–æ–ª–Ω–∞—è –ª—É–Ω–∞": {
        "emoji": "üåï",
        "tips": ["üìù –ó–∞–ø–∏—à–∏—Ç–µ –∏–¥–µ–∏", "üßò –ú—è–≥–∫–∞—è –º–µ–¥–∏—Ç–∞—Ü–∏—è", "üåô –°–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ª—É–Ω—É –æ—Ñ–ª–∞–π–Ω", "üìö 10 –º–∏–Ω—É—Ç —á—Ç–µ–Ω–∏—è"],
    },
    "–º–∏–Ω–∏-–ø–∞—Ä–∞–¥ –ø–ª–∞–Ω–µ—Ç": {
        "emoji": "‚ú®",
        "tips": ["üî≠ –†–∞—Å—Å–≤–µ—Ç–Ω–æ–µ –Ω–µ–±–æ", "üì∏ –§–æ—Ç–æ –∑–∞–∫–∞—Ç–∞", "ü§î –ú–∏–Ω—É—Ç–∫–∞ —Å–æ–∑–µ—Ä—Ü–∞–Ω–∏—è", "üé∂ –°–ø–æ–∫–æ–π–Ω–∞—è –º—É–∑—ã–∫–∞ –≤–µ—á–µ—Ä–æ–º"],
    },
}

ASTRO_HEALTH_FALLBACK: List[str] = [
    "üí§ –†–µ–∂–∏–º —Å–Ω–∞: –≤ –ø–æ—Å—Ç–µ–ª—å –¥–æ 23:00",
    "ü•¶ –ë–æ–ª—å—à–µ –æ–≤–æ—â–µ–π –∏ –∑–µ–ª–µ–Ω–∏",
    "ü•õ –¢—ë–ø–ª–æ–µ –º–æ–ª–æ–∫–æ/—á–∞–π –ø–µ—Ä–µ–¥ —Å–Ω–æ–º",
    "üßò –õ—ë–≥–∫–∞—è —Ä–∞—Å—Ç—è–∂–∫–∞ —É—Ç—Ä–æ–º/–≤–µ—á–µ—Ä–æ–º",
    "üö∂ 20 –º–∏–Ω—É—Ç –ø—Ä–æ–≥—É–ª–∫–∏ –≤ –¥–µ–Ω—å",
]

# ‚îÄ‚îÄ –ø—É–±–ª–∏—á–Ω–æ–µ API –¥–ª—è –±–ª–æ–∫–∞ ¬´–í—ã–≤–æ–¥/–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏¬ª ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def gpt_blurb(culprit: str) -> Tuple[str, List[str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (summary, tips[3]).
    –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–ª—é—á–µ–π –ø—Ä–æ—Å–∏—Ç LLM; –∏–Ω–∞—á–µ ‚Äî —Ñ–æ–ª–ª–±—ç–∫ –∏–∑ —Å–ª–æ–≤–∞—Ä–µ–π.
    """
    culprit = (culprit or "").strip()
    culprit_lower = culprit.lower()

    def _make_prompt(astro: bool) -> str:
        if astro:
            return (
                f"–î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π health coach —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ –∏ –ª—é–±–∏—Ç —É–¥–∏–≤–ª—è—Ç—å"
                f"–ù–∞–ø–∏—à–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ¬´–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}!¬ª. "
                f"–ü–æ—Å–ª–µ —Ç–æ—á–∫–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ–∑–∏—Ç–∏–≤ ‚â§12 —Å–ª–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤.  –ù–µ –ø–∏—à–∏ —Å–∞–º–æ —Å–ª–æ–≤–æ —Å–æ–≤–µ—Ç."
                f"–ó–∞—Ç–µ–º –¥–∞–π —Ä–æ–≤–Ω–æ 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—Å–æ–Ω, –ø–∏—Ç–∞–Ω–∏–µ, –¥—ã—Ö–∞–Ω–∏–µ/–ª—ë–≥–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å) ‚â§12 —Å–ª–æ–≤ —Å —ç–º–æ–¥–∑–∏. "
                f"–û—Ç–≤–µ—Ç ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–∞–º."
            )
        else:
            return (
                f"–î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ –æ–ø—ã—Ç–Ω—ã–π health coach —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ –∏ –ª—é–±–∏—Ç —É–¥–∏–≤–ª—è—Ç—å"
                f"–ù–∞–ø–∏—à–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ¬´–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}!¬ª. "
                f"–ü–æ—Å–ª–µ —Ç–æ—á–∫–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ–∑–∏—Ç–∏–≤ ‚â§12 —Å–ª–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤.  –ù–µ –ø–∏—à–∏ —Å–∞–º–æ —Å–ª–æ–≤–æ —Å–æ–≤–µ—Ç."
                f"–ó–∞—Ç–µ–º –¥–∞–π —Ä–æ–≤–Ω–æ 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø–∏—Ç–∞–Ω–∏–µ, —Å–æ–Ω, –ª—ë–≥–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å) ‚â§12 —Å–ª–æ–≤ —Å —ç–º–æ–¥–∑–∏. "
                f"–û—Ç–≤–µ—Ç ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–∞–º."
            )

    def _from_lines(lines: List[str], fallback_pool: List[str]) -> Tuple[str, List[str]]:
        summary = lines[0] if lines else f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ"
        tips = [ln for ln in lines[1:] if ln][:3]
        if len(tips) < 3:
            remain = [t for t in fallback_pool if t not in tips]
            tips += random.sample(remain, min(3 - len(tips), len(remain))) if remain else []
        return summary, tips[:3]

    # 1) ¬´–ü–æ–≥–æ–¥–Ω—ã–π¬ª —Ñ–∞–∫—Ç–æ—Ä –∏–∑ —Å–ª–æ–≤–∞—Ä—è
    if culprit_lower in CULPRITS:
        tips_pool = CULPRITS[culprit_lower]["tips"]
        text = gpt_complete(prompt=_make_prompt(astro=False), system=None, temperature=0.7, max_tokens=500)
        if not text:
            return (f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ",
                    random.sample(tips_pool, min(3, len(tips_pool))))
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(lines, tips_pool)

    # 2) –ê—Å—Ç—Ä–æ-—Ñ–∞–∫—Ç–æ—Ä
    astro = any(k in culprit_lower for k in ("–ª—É–Ω–∞", "–Ω–æ–≤–æ–ª—É–Ω–∏–µ", "–ø–æ–ª–Ω–æ–ª—É–Ω–∏–µ", "—á–µ—Ç–≤–µ—Ä—Ç—å"))
    if astro:
        text = gpt_complete(prompt=_make_prompt(astro=True), system=None, temperature=0.7, max_tokens=500)
        if not text:
            return (f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ",
                    random.sample(ASTRO_HEALTH_FALLBACK, 3))
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(lines, ASTRO_HEALTH_FALLBACK)

    # 3) –û–±—â–∏–π —Å–ª—É—á–∞–π
    text = gpt_complete(prompt=_make_prompt(astro=True), system=None, temperature=0.7, max_tokens=500)
    if not text:
        return (f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ",
                random.sample(ASTRO_HEALTH_FALLBACK, 3))
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    fallback_pool = ASTRO_HEALTH_FALLBACK + sum((c["tips"] for c in CULPRITS.values()), [])
    return _from_lines(lines, fallback_pool)
