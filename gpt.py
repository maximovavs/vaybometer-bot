#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
gpt.py

Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ LLM Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Â«Ğ’Ñ‹Ğ²Ğ¾Ğ´/Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸Â».

ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²: Gemini â†’ Groq â†’ OpenAI (Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ LLM_ORDER="gemini,groq,openai").
Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ„ĞµĞ¹Ğ»Ğ¾Ğ²ĞµÑ€: Ğ½Ğ° 404/429/insufficient_quota Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñƒ Ğ±ĞµĞ· Ñ€ĞµÑ‚Ñ€Ğ°ĞµĞ² SDK.

ENV:
    GEMINI_API_KEY
    GROQ_API_KEY
    OPENAI_API_KEY

ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ:
    GEMINI_MODEL   (default: "gemini-1.5-flash")
    OPENAI_MODEL   (default: "gpt-4o-mini")
    GROQ_MODELS    (comma-separated; Ğ¿ĞµÑ€Ğ²Ğ°Ñ â€” Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ°Ñ)
    LLM_ORDER      (comma-separated: "gemini,groq,openai")

ĞŸÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸:
    - gpt_complete(prompt, system=None, temperature=0.7, max_tokens=600) -> str
    - gpt_blurb(culprit) -> (summary: str, tips: List[str])

Ğ’Ğ°Ğ¶Ğ½Ğ¾:
    - ĞšĞ»ÑÑ‡Ğ¸ ĞĞ˜ĞšĞĞ“Ğ”Ğ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚.
    - Ğ”Ğ»Ñ Gemini Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ REST v1beta (+ systemInstruction).
"""

from __future__ import annotations
import os
import re
import random
import logging
from typing import Tuple, List, Optional

log = logging.getLogger(__name__)

# â”€â”€ SDK / HTTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from openai import OpenAI  # Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸ Ğ´Ğ»Ñ Groq (OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API)
except ImportError:
    OpenAI = None  # type: ignore

try:
    import requests  # Gemini Ñ‡ĞµÑ€ĞµĞ· REST
except Exception:
    requests = None  # type: ignore

# â”€â”€ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_KEY = os.getenv("OPENAI_API_KEY") or ""
GEMINI_KEY = os.getenv("GEMINI_API_KEY") or ""
GROQ_KEY   = os.getenv("GROQ_API_KEY") or ""

_default_order = ["gemini", "groq", "openai"]
PROVIDER_ORDER = [
    p.strip().lower()
    for p in (os.getenv("LLM_ORDER") or ",".join(_default_order)).split(",")
    if p.strip()
]

# â”€â”€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğµ env) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")  # ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ±ĞµĞ· -latest
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

_env_groq_models = [m.strip() for m in (os.getenv("GROQ_MODELS") or "").split(",") if m.strip()]
GROQ_MODELS = _env_groq_models or [
    "moonshotai/kimi-k2-instruct-0905",  # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ°Ñ
    "llama-3.3-70b-versatile",
    "llama-3.1-70b-versatile",
    "llama-3.1-8b-instant",
    "gemma2-9b-it",
    "qwen/qwen3-32b",
    "deepseek-r1-distill-llama-70b",  # Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ <think>...</think>
]

# â”€â”€ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _openai_client() -> Optional["OpenAI"]:
    """ĞšĞ»Ğ¸ĞµĞ½Ñ‚ OpenAI Ğ±ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ñ€ĞµÑ‚Ñ€Ğ°ĞµĞ² â€” Ğ½Ğ° 429 Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ´Ğ°Ğ»ÑŒÑˆĞµ."""
    if not OPENAI_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=OPENAI_KEY, timeout=20.0, max_retries=0)
    except Exception as e:
        log.warning("[openai] client init error: %s", e)
        return None

def _groq_client() -> Optional["OpenAI"]:
    """OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Groq."""
    if not GROQ_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=GROQ_KEY, base_url="https://api.groq.com/openai/v1", timeout=25.0, max_retries=0)
    except Exception as e:
        log.warning("[groq] client init error: %s", e)
        return None

# â”€â”€ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _strip_think(text: str) -> str:
    """Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ/ĞºĞ¾Ğ´Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ‹."""
    if not text:
        return ""
    # Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ‚ĞµĞ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
    text = re.sub(r"(?is)<(think|reasoning|scratchpad)>.*?</\1>", "", text)
    # Ğ¢Ñ€Ğ¾Ğ¹Ğ½Ñ‹Ğµ ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸-ĞºĞ¾Ğ´Ğ±Ğ»Ğ¾ĞºĞ¸
    text = re.sub(r"(?is)```(?:\w+)?\n(.*?)```", r"\1", text)
    # Ğ¤Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´Ğ° <foo>...</foo> Ğ±ĞµĞ· whitelisted Ñ‚ĞµĞ³Ğ¾Ğ²
    text = re.sub(r"(?is)</?([a-z][a-z0-9_-]{0,20})>", "", text)
    # Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²
    text = re.sub(r"(.)\1{4,}", r"\1\1\1", text)
    return text.strip()

def _shorten(s: str, n: int = 400) -> str:
    s = s or ""
    return s if len(s) <= n else s[:n] + "â€¦"

# â”€â”€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gpt_complete(
    prompt: str,
    system: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 600,
) -> str:
    """
    Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² LLM. ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸: Gemini â†’ Groq â†’ OpenAI (Ğ¿Ğ¾ PROVIDER_ORDER).
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ text Ğ¸Ğ»Ğ¸ "".
    """
    text = ""

    # Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ OpenAI (Ğ´Ğ»Ñ Groq/OpenAI)
    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    # 1) Gemini (REST v1beta)
    if "gemini" in PROVIDER_ORDER and not text and GEMINI_KEY and requests:
        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
            params = {"key": GEMINI_KEY}
            payload: dict = {
                "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                "generationConfig": {"temperature": temperature, "maxOutputTokens": max_tokens},
            }
            if system:
                # Ğ‘Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾, Ñ‡ĞµĞ¼ ÑĞºĞ»ĞµĞ¹ĞºĞ° system+user
                payload["systemInstruction"] = {"role": "system", "parts": [{"text": system}]}

            resp = requests.post(url, params=params, json=payload, timeout=30)
            if resp.status_code == 200:
                data = resp.json() or {}

                # Safety-Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
                fb = (data.get("promptFeedback") or {})
                block_reason = fb.get("blockReason")
                if block_reason:
                    log.warning("[gemini %s] prompt blocked: %s", GEMINI_MODEL, block_reason)
                else:
                    candidates = data.get("candidates") or []
                    if candidates:
                        cand = candidates[0] or {}
                        finish = cand.get("finishReason")
                        if finish in (None, "STOP"):
                            content = cand.get("content") or {}
                            parts = content.get("parts") or []
                            text = "".join(p.get("text", "") for p in parts).strip()
                            if text:
                                log.info("[gemini %s] ok (%d chars)", GEMINI_MODEL, len(text))
                        else:
                            log.warning("[gemini %s] finishReason=%s", GEMINI_MODEL, finish)
                    else:
                        log.warning("[gemini %s] no candidates", GEMINI_MODEL)
            else:
                code = resp.status_code
                body = _shorten(resp.text)
                # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ„ĞµĞ¹Ğ»Ğ¾Ğ²ĞµÑ€ Ğ½Ğ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑÑ‹
                if code in (404, 409, 413, 422, 429, 500, 503):
                    log.warning("[gemini %s] http %s: %s", GEMINI_MODEL, code, body)
                else:
                    log.warning("[gemini %s] http %s: %s", GEMINI_MODEL, code, body)
        except Exception as e:
            log.warning("[gemini %s] exception: %s", GEMINI_MODEL, e)

    # 2) Groq (OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹)
    if "groq" in PROVIDER_ORDER and not text:
        cli = _groq_client()
        if cli:
            for mdl in GROQ_MODELS:
                try:
                    r = cli.chat.completions.create(
                        model=mdl,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                    text = (r.choices[0].message.content or "").strip()
                    if text:
                        log.info("[groq %s] ok (%d chars)", mdl, len(text))
                        break
                except Exception as e:
                    msg = str(e).lower()
                    if "decommissioned" in msg or ("model" in msg and "not found" in msg):
                        log.warning("[groq %s] model not found/decommissioned, trying next", mdl)
                        continue
                    if "rate limit" in msg or "429" in msg or "quota" in msg:
                        log.warning("[groq %s] rate limit/quota, trying next", mdl)
                        continue
                    log.warning("[groq %s] error: %s", mdl, _shorten(str(e)))
                    continue

    # 3) OpenAI
    if "openai" in PROVIDER_ORDER and not text:
        cli = _openai_client()
        if cli:
            try:
                r = cli.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                text = (r.choices[0].message.content or "").strip()
                if text:
                    log.info("[openai %s] ok (%d chars)", OPENAI_MODEL, len(text))
            except Exception as e:
                msg = str(e).lower()
                if any(k in msg for k in ("rate limit", "insufficient_quota", "429")):
                    log.warning("[openai %s] rate limit/quota: %s", OPENAI_MODEL, _shorten(str(e)))
                else:
                    log.warning("[openai %s] error: %s", OPENAI_MODEL, _shorten(str(e)))

    return _strip_think(text or "")

# â”€â”€ Ñ„Ğ¾Ğ»Ğ±ÑĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CULPRITS = {
    "Ñ‚ÑƒĞ¼Ğ°Ğ½": {
        "emoji": "ğŸŒ",
        "tips": [
            "ğŸ”¦ Ğ¡Ğ²ĞµÑ‚Ğ»Ğ°Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ğ° Ğ¸ Ñ„Ğ¾Ğ½Ğ°Ñ€ÑŒ",
            "ğŸš— Ğ’Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ğ°ĞºĞºÑƒÑ€Ğ°Ñ‚Ğ½Ğ¾",
            "â° ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾ĞµĞ·Ğ´ĞºĞ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ",
            "ğŸ•¶ï¸ ĞÑ‡ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ»Ğ¸ĞºĞ¾Ğ²",
        ],
    },
    "Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ±ÑƒÑ€Ğ¸": {
        "emoji": "ğŸ§²",
        "tips": [
            "ğŸ§˜ 5-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ğ°Ñ Ğ´Ñ‹Ñ…Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ°",
            "ğŸŒ¿ Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ²ÑĞ½Ğ¾Ğ¹ Ñ‡Ğ°Ğ¹",
            "ğŸ™… ĞœĞµĞ½ÑŒÑˆĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞºÑ€Ğ°Ğ½Ğ¾Ğ²",
            "ğŸ˜Œ Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ñ€Ğ°ÑÑ‚ÑĞ¶ĞºĞ° Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
    "Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ": {
        "emoji": "ğŸŒ¡ï¸",
        "tips": [
            "ğŸ’§ ĞŸĞµĞ¹Ñ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ¾Ğ´Ñ‹",
            "ğŸ˜´ 20-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ´Ñ‹Ñ…",
            "ğŸ¤¸ ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ñ€ÑĞ´ĞºĞ° ÑƒÑ‚Ñ€Ğ¾Ğ¼",
            "ğŸ¥— ĞœĞµĞ½ÑŒÑˆĞµ ÑĞ¾Ğ»Ğ¸ Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
    "ÑˆĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ‚ĞµÑ€": {
        "emoji": "ğŸ’¨",
        "tips": [
            "ğŸ§£ Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚Ğµ ÑˆĞ°Ñ€Ñ„",
            "ğŸš¶ ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ³ÑƒĞ»ĞºĞ°",
            "ğŸ•¶ï¸ Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚Ğµ Ğ³Ğ»Ğ°Ğ·Ğ° Ğ¾Ñ‚ Ğ¿Ñ‹Ğ»Ğ¸",
            "ğŸŒ³ Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹Ñ‚Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²",
        ],
    },
    "Ğ¶Ğ°Ñ€Ğ°": {
        "emoji": "ğŸ”¥",
        "tips": [
            "ğŸ’¦ Ğ‘ÑƒÑ‚Ñ‹Ğ»ĞºĞ° Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ñ€ÑƒĞºĞ¾Ğ¹",
            "ğŸ§¢ Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑƒĞ±Ğ¾Ñ€ Ğ¸ Ñ‚ĞµĞ½ÑŒ",
            "â± Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ»ÑƒĞ´Ğ½Ñ",
            "â„ï¸ ĞÑ…Ğ»Ğ°Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑ",
        ],
    },
    "ÑÑ‹Ñ€Ğ¾ÑÑ‚ÑŒ": {
        "emoji": "ğŸ’§",
        "tips": [
            "ğŸ‘Ÿ Ğ¡Ğ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±ÑƒĞ²ÑŒ",
            "ğŸŒ‚ ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ·Ğ¾Ğ½Ñ‚",
            "ğŸŒ¬ï¸ ĞŸÑ€Ğ¾Ğ²ĞµÑ‚Ñ€Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ´Ğ¾Ğ¼",
            "ğŸ§¥ Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¼Ğ¾ĞºĞ°ĞµĞ¼Ğ°Ñ ĞºÑƒÑ€Ñ‚ĞºĞ°",
        ],
    },
    "Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ»ÑƒĞ½Ğ°": {
        "emoji": "ğŸŒ•",
        "tips": [
            "ğŸ“ Ğ—Ğ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ½Ğ¾Ğ¼",
            "ğŸ§˜ ĞœÑĞ³ĞºĞ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ",
            "ğŸŒ™ ĞœĞ¸Ğ½ÑƒÑ‚ĞºĞ° Ğ±ĞµĞ· Ğ³Ğ°Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²",
            "ğŸ“š ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ",
        ],
    },
    "Ğ¼Ğ¸Ğ½Ğ¸-Ğ¿Ğ°Ñ€Ğ°Ğ´ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚": {
        "emoji": "âœ¨",
        "tips": [
            "ğŸ”­ ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾ Ğ½Ğ° Ñ€Ğ°ÑÑĞ²ĞµÑ‚Ğµ",
            "ğŸ“¸ Ğ¡Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ·Ğ°ĞºĞ°Ñ‚",
            "ğŸ¤” ĞœĞ¸Ğ½ÑƒÑ‚ĞºĞ° Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹",
            "ğŸ¶ Ğ¡Ğ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ°Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ° Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
}

ASTRO_HEALTH_FALLBACK: List[str] = [
    "ğŸ’¤ Ğ›Ğ¾Ğ¶Ğ¸Ñ‚ĞµÑÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ 23:00",
    "ğŸ¥¦ Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ·ĞµĞ»ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ğ²Ğ¾Ñ‰ĞµĞ¹",
    "ğŸš¶ 20 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ³ÑƒĞ»ĞºĞ¸",
    "ğŸ«– Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğ¹ Ğ½Ğ°ÑÑ‚Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ½Ğ¾Ğ¼",
    "ğŸ§˜ 3 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ 4-7-8",
]

# â”€â”€ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Â«Ğ’Ñ‹Ğ²Ğ¾Ğ´/Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸Â» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gpt_blurb(culprit: str) -> Tuple[str, List[str]]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ (summary: str, tips: List[str]).
    Ğ•ÑĞ»Ğ¸ LLM Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ„Ğ¾Ğ»Ğ±ÑĞº-ÑĞ¿Ğ¸ÑĞºĞ¸.
    Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ° Ğ·Ğ´ĞµÑÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ°Ñ (â‰ˆ0.2), ĞºĞ°Ğº Ğ¿Ğ¾ Ğ¢Ğ—.
    """
    culprit_lower = (culprit or "").lower().strip()

    def _make_prompt(cul: str, astro: bool) -> str:
        base = (
            "Ğ¢Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ health-ĞºĞ¾ÑƒÑ‡: Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹, ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹, Ğ±ĞµĞ· ÑˆÑ‚Ğ°Ğ¼Ğ¿Ğ¾Ğ². "
            "Ğ”Ğ°Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼, Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼."
        )
        tail = (
            f"1) ĞŸĞµÑ€Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°: Â«Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {cul}!Â». "
            "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ² (â‰¤12 ÑĞ»Ğ¾Ğ²). "
            "2) Ğ”Ğ°Ğ»ĞµĞµ Ñ€Ğ¾Ğ²Ğ½Ğ¾ 3 ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ (â‰¤12 ÑĞ»Ğ¾Ğ²) Ñ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸. "
            "Ğ¢ĞµĞ¼Ñ‹: ÑĞ¾Ğ½, Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğµ, Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ/Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ğµ."
        )
        if astro:
            tail += " Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ†Ğ¸ĞºĞ»Ğ°Ğ¼ Ğ¸ Ğ¼ÑĞ³ĞºĞ¸Ğ¹ Ñ‚Ğ¾Ğ½."
        return base + " " + tail

    def _from_lines(cul: str, lines: List[str], fallback_pool: List[str]) -> Tuple[str, List[str]]:
        summary = lines[0] if lines else f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {cul}! ğŸ˜‰"
        tips = [ln for ln in lines[1:] if ln][:3]
        if len(tips) < 3:
            remain = [t for t in fallback_pool if t not in tips]
            if remain:
                # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ 3 â€” Ğ´Ğ¾Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼, Ğ½Ğ¾ Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, ĞµÑĞ»Ğ¸ Ğ¿ÑƒĞ» Ğ¿ÑƒÑÑ‚
                k = min(3 - len(tips), len(remain))
                tips += random.sample(remain, k)
        return summary, tips[:3]

    # 1) Â«ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğ¹Â» Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€
    if culprit_lower in CULPRITS:
        pool = CULPRITS[culprit_lower]["tips"]
        text = gpt_complete(prompt=_make_prompt(culprit, astro=False), system=None, temperature=0.2, max_tokens=240)
        if not text:
            # LLM Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€” Ñ„Ğ¾Ğ»Ğ±ÑĞº
            k = min(3, len(pool))
            return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(pool, k) if k else []
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, pool)

    # 2) Â«ĞÑÑ‚Ñ€Ğ¾Â» Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€
    is_astro = any(k in culprit_lower for k in ["Ğ»ÑƒĞ½Ğ°", "Ğ½Ğ¾Ğ²Ğ¾Ğ»ÑƒĞ½Ğ¸Ğµ", "Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ»ÑƒĞ½Ğ¸Ğµ", "Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚ÑŒ"])
    if is_astro:
        text = gpt_complete(prompt=_make_prompt(culprit, astro=True), system=None, temperature=0.2, max_tokens=240)
        if not text:
            k = min(3, len(ASTRO_HEALTH_FALLBACK))
            return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(ASTRO_HEALTH_FALLBACK, k)
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, ASTRO_HEALTH_FALLBACK)

    # 3) ĞĞ±Ñ‰Ğ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹
    text = gpt_complete(prompt=_make_prompt(culprit, astro=True), system=None, temperature=0.2, max_tokens=240)
    if not text:
        k = min(3, len(ASTRO_HEALTH_FALLBACK))
        return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(ASTRO_HEALTH_FALLBACK, k)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    fallback_pool = ASTRO_HEALTH_FALLBACK + sum((c["tips"] for c in CULPRITS.values()), [])
    return _from_lines(culprit, lines, fallback_pool)
