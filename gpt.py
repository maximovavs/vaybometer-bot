#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
gpt.py

Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ LLM Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Â«Ğ’Ñ‹Ğ²Ğ¾Ğ´/Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸Â».

ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ): Gemini â†’ Groq â†’ OpenAI.
- ĞĞ° 404/429/Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ĞºĞ²Ğ¾Ñ‚Ñ‹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ.
- ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ:
    GEMINI_MODEL   (default: "gemini-1.5-flash-latest")
    OPENAI_MODEL   (default: "gpt-4o-mini")
    GROQ_MODELS    (comma-separated; Ğ¿ĞµÑ€Ğ²Ğ°Ñ â€” Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ°Ñ)
- ĞšĞ»ÑÑ‡Ğ¸ (OPENAI_API_KEY / GEMINI_API_KEY / GROQ_API_KEY) ĞĞ˜ĞšĞĞ“Ğ”Ğ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚.

ĞŸÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸:
- gpt_complete(prompt, system, temperature, max_tokens) -> str
- gpt_blurb(culprit) -> (summary: str, tips: List[str])

Ğ¤Ğ¾Ğ»Ğ±ÑĞºĞ¸:
- CULPRITS, ASTRO_HEALTH_FALLBACK â€” ĞµÑĞ»Ğ¸ LLM Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½.
"""

from __future__ import annotations
import os
import re
import random
import logging
from typing import Tuple, List, Optional

log = logging.getLogger(__name__)

# â”€â”€ SDK / HTTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from openai import OpenAI   # Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¸ Ğ´Ğ»Ñ Groq (ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API)
except ImportError:
    OpenAI = None

try:
    import requests             # Gemini Ñ‡ĞµÑ€ĞµĞ· REST
except Exception:
    requests = None

# â”€â”€ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_KEY = os.getenv("OPENAI_API_KEY") or ""
GEMINI_KEY = os.getenv("GEMINI_API_KEY") or ""
GROQ_KEY   = os.getenv("GROQ_API_KEY") or ""

# ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²: Gemini â†’ Groq â†’ OpenAI (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ LLM_ORDER="gemini,groq,openai")
_default_order = ["gemini", "groq", "openai"]
PROVIDER_ORDER = [p.strip().lower() for p in (os.getenv("LLM_ORDER") or ",".join(_default_order)).split(",") if p.strip()]

# ĞœĞ¾Ğ´ĞµĞ»Ğ¸ (Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼Ñ‹ env-Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash-latest")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

_env_groq_models = [m.strip() for m in (os.getenv("GROQ_MODELS") or "").split(",") if m.strip()]
GROQ_MODELS = _env_groq_models or [
    "moonshotai/kimi-k2-instruct-0905",  # Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ² Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğµ
    "llama-3.3-70b-versatile",
    "llama-3.1-70b-versatile",
    "llama-3.1-8b-instant",
    "gemma2-9b-it",
    "qwen/qwen3-32b",
    "deepseek-r1-distill-llama-70b",     # Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ <think>...</think>
]

# â”€â”€ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _openai_client() -> Optional["OpenAI"]:
    """ĞšĞ»Ğ¸ĞµĞ½Ñ‚ OpenAI Ğ±ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ñ€ĞµÑ‚Ñ€Ğ°ĞµĞ² â€” Ğ½Ğ° 429 Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ´Ğ°Ğ»ÑŒÑˆĞµ."""
    if not OPENAI_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=OPENAI_KEY, timeout=20.0, max_retries=0)
    except Exception as e:
        log.warning("OpenAI client init error: %s", e)
        return None

def _groq_client() -> Optional["OpenAI"]:
    """OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Groq."""
    if not GROQ_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=GROQ_KEY, base_url="https://api.groq.com/openai/v1", timeout=25.0, max_retries=0)
    except Exception as e:
        log.warning("Groq client init error: %s", e)
        return None

# â”€â”€ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _strip_think(text: str) -> str:
    """Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ <think>â€¦</think> Ğ¸ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ‹/Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹."""
    if not text:
        return ""
    text = re.sub(r"(?is)<think>.*?</think>", "", text).strip()
    # Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²
    text = re.sub(r"(.)\1{4,}", r"\1\1\1", text)
    return text.strip()

# â”€â”€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gpt_complete(
    prompt: str,
    system: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 600,
) -> str:
    """
    Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² LLM. ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸: Gemini â†’ Groq â†’ OpenAI (Ğ¿Ğ¾ PROVIDER_ORDER).
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ text Ğ¸Ğ»Ğ¸ "".
    """
    text = ""

    # Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ OpenAI
    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    # 1) Gemini (REST v1beta)
    if "gemini" in PROVIDER_ORDER and not text and GEMINI_KEY and requests:
        try:
            full_prompt = f"{system.strip()}\n\n{prompt}" if system else prompt
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
            params = {"key": GEMINI_KEY}
            payload = {
                "contents": [{"parts": [{"text": full_prompt}]}],
                "generationConfig": {"temperature": temperature, "maxOutputTokens": max_tokens}
            }
            resp = requests.post(url, params=params, json=payload, timeout=30)
            if resp.status_code == 200:
                data = resp.json() or {}
                # candidates[0].content.parts[*].text
                cand = (data.get("candidates") or [{}])[0]
                content = cand.get("content") or {}
                parts = content.get("parts") or []
                text = "".join(p.get("text", "") for p in parts).strip()
            else:
                log.warning("Gemini error %s: %s", resp.status_code, resp.text[:500])
        except Exception as e:
            log.warning("Gemini exception: %s", e)

    # 2) Groq (OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹)
    if "groq" in PROVIDER_ORDER and not text:
        cli = _groq_client()
        if cli:
            for mdl in GROQ_MODELS:
                try:
                    r = cli.chat.completions.create(
                        model=mdl,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                    text = (r.choices[0].message.content or "").strip()
                    if text:
                        break
                except Exception as e:
                    msg = str(e).lower()
                    if "decommissioned" in msg or ("model" in msg and "not found" in msg):
                        log.warning("Groq model %s not found/decommissioned, trying next.", mdl)
                        continue
                    if "rate limit" in msg or "429" in msg or "quota" in msg:
                        log.warning("Groq rate limit on %s, trying next.", mdl)
                        continue
                    log.warning("Groq error on %s: %s", mdl, e)
                    continue

    # 3) OpenAI
    if "openai" in PROVIDER_ORDER and not text:
        cli = _openai_client()
        if cli:
            try:
                r = cli.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                text = (r.choices[0].message.content or "").strip()
            except Exception as e:
                msg = str(e).lower()
                if any(k in msg for k in ("rate limit", "insufficient_quota", "429")):
                    log.warning("OpenAI error (skip): %s", e)
                else:
                    log.warning("OpenAI error: %s", e)

    return _strip_think(text or "")

# â”€â”€ Ñ„Ğ¾Ğ»Ğ±ÑĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CULPRITS = {
    "Ñ‚ÑƒĞ¼Ğ°Ğ½": {
        "emoji": "ğŸŒ",
        "tips": [
            "ğŸ”¦ Ğ¡Ğ²ĞµÑ‚Ğ»Ğ°Ñ Ğ¾Ğ´ĞµĞ¶Ğ´Ğ° Ğ¸ Ñ„Ğ¾Ğ½Ğ°Ñ€ÑŒ",
            "ğŸš— Ğ’Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ğ°ĞºĞºÑƒÑ€Ğ°Ñ‚Ğ½Ğ¾",
            "â° ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾ĞµĞ·Ğ´ĞºĞ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ",
            "ğŸ•¶ï¸ ĞÑ‡ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ±Ğ»Ğ¸ĞºĞ¾Ğ²",
        ],
    },
    "Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ±ÑƒÑ€Ğ¸": {
        "emoji": "ğŸ§²",
        "tips": [
            "ğŸ§˜ 5-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ğ°Ñ Ğ´Ñ‹Ñ…Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ°",
            "ğŸŒ¿ Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ²ÑĞ½Ğ¾Ğ¹ Ñ‡Ğ°Ğ¹",
            "ğŸ™… ĞœĞµĞ½ÑŒÑˆĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞºÑ€Ğ°Ğ½Ğ¾Ğ²",
            "ğŸ˜Œ Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ñ€Ğ°ÑÑ‚ÑĞ¶ĞºĞ° Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
    "Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ": {
        "emoji": "ğŸŒ¡ï¸",
        "tips": [
            "ğŸ’§ ĞŸĞµĞ¹Ñ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ¾Ğ´Ñ‹",
            "ğŸ˜´ 20-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ´Ñ‹Ñ…",
            "ğŸ¤¸ ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ñ€ÑĞ´ĞºĞ° ÑƒÑ‚Ñ€Ğ¾Ğ¼",
            "ğŸ¥— ĞœĞµĞ½ÑŒÑˆĞµ ÑĞ¾Ğ»Ğ¸ Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
    "ÑˆĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ‚ĞµÑ€": {
        "emoji": "ğŸ’¨",
        "tips": [
            "ğŸ§£ Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚Ğµ ÑˆĞ°Ñ€Ñ„",
            "ğŸš¶ ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ³ÑƒĞ»ĞºĞ°",
            "ğŸ•¶ï¸ Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚Ğµ Ğ³Ğ»Ğ°Ğ·Ğ° Ğ¾Ñ‚ Ğ¿Ñ‹Ğ»Ğ¸",
            "ğŸŒ³ Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹Ñ‚Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²",
        ],
    },
    "Ğ¶Ğ°Ñ€Ğ°": {
        "emoji": "ğŸ”¥",
        "tips": [
            "ğŸ’¦ Ğ‘ÑƒÑ‚Ñ‹Ğ»ĞºĞ° Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ñ€ÑƒĞºĞ¾Ğ¹",
            "ğŸ§¢ Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑƒĞ±Ğ¾Ñ€ Ğ¸ Ñ‚ĞµĞ½ÑŒ",
            "â± Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ»ÑƒĞ´Ğ½Ñ",
            "â„ï¸ ĞÑ…Ğ»Ğ°Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑ",
        ],
    },
    "ÑÑ‹Ñ€Ğ¾ÑÑ‚ÑŒ": {
        "emoji": "ğŸ’§",
        "tips": [
            "ğŸ‘Ÿ Ğ¡Ğ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±ÑƒĞ²ÑŒ",
            "ğŸŒ‚ ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ·Ğ¾Ğ½Ñ‚",
            "ğŸŒ¬ï¸ ĞŸÑ€Ğ¾Ğ²ĞµÑ‚Ñ€Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ´Ğ¾Ğ¼",
            "ğŸ§¥ Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¼Ğ¾ĞºĞ°ĞµĞ¼Ğ°Ñ ĞºÑƒÑ€Ñ‚ĞºĞ°",
        ],
    },
    "Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ»ÑƒĞ½Ğ°": {
        "emoji": "ğŸŒ•",
        "tips": [
            "ğŸ“ Ğ—Ğ°Ğ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ½Ğ¾Ğ¼",
            "ğŸ§˜ ĞœÑĞ³ĞºĞ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ",
            "ğŸŒ™ ĞœĞ¸Ğ½ÑƒÑ‚ĞºĞ° Ğ±ĞµĞ· Ğ³Ğ°Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²",
            "ğŸ“š ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ",
        ],
    },
    "Ğ¼Ğ¸Ğ½Ğ¸-Ğ¿Ğ°Ñ€Ğ°Ğ´ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚": {
        "emoji": "âœ¨",
        "tips": [
            "ğŸ”­ ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾ Ğ½Ğ° Ñ€Ğ°ÑÑĞ²ĞµÑ‚Ğµ",
            "ğŸ“¸ Ğ¡Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ·Ğ°ĞºĞ°Ñ‚",
            "ğŸ¤” ĞœĞ¸Ğ½ÑƒÑ‚ĞºĞ° Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹",
            "ğŸ¶ Ğ¡Ğ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ°Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ° Ğ²ĞµÑ‡ĞµÑ€Ğ¾Ğ¼",
        ],
    },
}

ASTRO_HEALTH_FALLBACK: List[str] = [
    "ğŸ’¤ Ğ›Ğ¾Ğ¶Ğ¸Ñ‚ĞµÑÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ 23:00",
    "ğŸ¥¦ Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ·ĞµĞ»ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ğ²Ğ¾Ñ‰ĞµĞ¹",
    "ğŸš¶ 20 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ³ÑƒĞ»ĞºĞ¸",
    "ğŸ«– Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğ¹ Ğ½Ğ°ÑÑ‚Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ½Ğ¾Ğ¼",
    "ğŸ§˜ 3 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ñ 4-7-8",
]

# â”€â”€ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Â«Ğ’Ñ‹Ğ²Ğ¾Ğ´/Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸Â» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gpt_blurb(culprit: str) -> Tuple[str, List[str]]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ (summary: str, tips: List[str]).
    Ğ•ÑĞ»Ğ¸ LLM Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ„Ğ¾Ğ»Ğ±ÑĞº-ÑĞ¿Ğ¸ÑĞºĞ¸.
    """
    culprit_lower = (culprit or "").lower().strip()

    def _make_prompt(cul: str, astro: bool) -> str:
        base = (
            "Ğ¢Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ health-ĞºĞ¾ÑƒÑ‡: Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹, ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹, Ğ±ĞµĞ· ÑˆÑ‚Ğ°Ğ¼Ğ¿Ğ¾Ğ². "
            "Ğ”Ğ°Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼, Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼."
        )
        tail = (
            f"1) ĞŸĞµÑ€Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°: Â«Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {cul}!Â». "
            "Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ² (â‰¤12 ÑĞ»Ğ¾Ğ²). "
            "2) Ğ”Ğ°Ğ»ĞµĞµ Ñ€Ğ¾Ğ²Ğ½Ğ¾ 3 ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ (â‰¤12 ÑĞ»Ğ¾Ğ²) Ñ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸. "
            "Ğ¢ĞµĞ¼Ñ‹: ÑĞ¾Ğ½, Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ğµ, Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ/Ğ´Ñ‹Ñ…Ğ°Ğ½Ğ¸Ğµ."
        )
        if astro:
            tail += " Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ†Ğ¸ĞºĞ»Ğ°Ğ¼ Ğ¸ Ğ¼ÑĞ³ĞºĞ¸Ğ¹ Ñ‚Ğ¾Ğ½."
        return base + " " + tail

    def _from_lines(cul: str, lines: List[str], fallback_pool: List[str]) -> Tuple[str, List[str]]:
        summary = lines[0] if lines else f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {cul}! ğŸ˜‰"
        tips = [ln for ln in lines[1:] if ln][:3]
        if len(tips) < 3:
            remain = [t for t in fallback_pool if t not in tips]
            tips += random.sample(remain, min(3 - len(tips), len(remain))) if remain else []
        return summary, tips[:3]

    # 1) Â«ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğ¹Â» Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€
    if culprit_lower in CULPRITS:
        pool = CULPRITS[culprit_lower]["tips"]
        text = gpt_complete(prompt=_make_prompt(culprit, astro=False), system=None, temperature=0.6, max_tokens=240)
        if not text:
            return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(pool, min(3, len(pool)))
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, pool)

    # 2) Â«ĞÑÑ‚Ñ€Ğ¾Â» Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€
    is_astro = any(k in culprit_lower for k in ["Ğ»ÑƒĞ½Ğ°", "Ğ½Ğ¾Ğ²Ğ¾Ğ»ÑƒĞ½Ğ¸Ğµ", "Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ»ÑƒĞ½Ğ¸Ğµ", "Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚ÑŒ"])
    if is_astro:
        text = gpt_complete(prompt=_make_prompt(culprit, astro=True), system=None, temperature=0.6, max_tokens=240)
        if not text:
            return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(ASTRO_HEALTH_FALLBACK, 3)
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, ASTRO_HEALTH_FALLBACK)

    # 3) ĞĞ±Ñ‰Ğ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹
    text = gpt_complete(prompt=_make_prompt(culprit, astro=True), system=None, temperature=0.6, max_tokens=240)
    if not text:
        return f"Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ° Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğµ Ñ‚Ğ°Ğº, Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ {culprit}! ğŸ˜‰", random.sample(ASTRO_HEALTH_FALLBACK, 3)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    fallback_pool = ASTRO_HEALTH_FALLBACK + sum((c["tips"] for c in CULPRITS.values()), [])
    return _from_lines(culprit, lines, fallback_pool)
